diff --git a/net/ipv4/tcp_bbrplus.c.orig b/net/ipv4/tcp_bbrplus.c
index 54eec33..cbbf311 100644
--- a/net/ipv4/tcp_bbrplus.c.orig
+++ b/net/ipv4/tcp_bbrplus.c
@@ -100,7 +100,8 @@ struct bbr {
 		round_start:1,	     /* start of packet-timed tx->ack round? */
 		idle_restart:1,	     /* restarting after idle? */
 		probe_rtt_round_done:1,  /* a BBR_PROBE_RTT round at 4 pkts? */
-		unused:13,
+		cycle_len:4,
+		unused:9,
 		lt_is_sampling:1,    /* taking long-term ("LT") samples now? */
 		lt_rtt_cnt:7,	     /* round trips in long-term interval */
 		lt_use_bw:1;	     /* use lt_bw as our bw estimate? */
@@ -158,6 +159,13 @@ static const int bbr_high_gain  = BBR_UNIT * 2885 / 1000 + 1;
 static const int bbr_drain_gain = BBR_UNIT * 1000 / 2885;
 /* The gain for deriving steady-state cwnd tolerates delayed/stretched ACKs: */
 static const int bbr_cwnd_gain  = BBR_UNIT * 2;
+
+enum bbr_pacing_gain_phase {
+    BBR_BW_PROBE_UP     = 0,
+    BBR_BW_PROBE_DOWN   = 1,
+    BBR_BW_PROBE_CRUISE = 2,
+};
+
 /* The pacing_gain values for the PROBE_BW gain cycle, to discover/share bw: */
 static const int bbr_pacing_gain[] = {
 	BBR_UNIT * 5 / 4,	/* probe for more available bw */
@@ -203,6 +211,22 @@ static const u32 bbr_extra_acked_max_us = 100 * 1000;
 
 static void bbr_check_probe_rtt_done(struct sock *sk);
 
+/* Each cycle, try to hold sub-unity gain until inflight <= BDP. */
+static const bool bbr_drain_to_target = true;   /* default: enabled */
+
+/* Does at least the first segment of SKB fit into the send window? */
+static bool tcp_snd_wnd_test(const struct tcp_sock *tp,
+                 const struct sk_buff *skb,
+                 unsigned int cur_mss)
+{
+    u32 end_seq = TCP_SKB_CB(skb)->end_seq;
+
+    if (skb->len > cur_mss)
+        end_seq = TCP_SKB_CB(skb)->seq + cur_mss;
+
+    return !after(end_seq, tcp_wnd_end(tp));
+}
+
 /* Do we estimate that STARTUP filled the pipe? */
 static bool bbr_full_bw_reached(const struct sock *sk)
 {
@@ -419,6 +443,66 @@ static u32 bbr_inflight(struct sock *sk, u32 bw, int gain)
 	return inflight;
 }
 
+static void bbr_set_cycle_idx(struct sock *sk, int cycle_idx)
+{
+    struct bbr *bbr = inet_csk_ca(sk);
+    bbr->cycle_idx = cycle_idx;
+    bbr->pacing_gain = bbr->lt_use_bw ?
+                            BBR_UNIT : bbr_pacing_gain[bbr->cycle_idx];
+}
+
+static void bbr_drain_to_target_cycling(struct sock *sk,
+                                                    const struct rate_sample *rs)
+{
+    struct tcp_sock *tp = tcp_sk(sk);
+    struct bbr *bbr = inet_csk_ca(sk);
+    u32 elapsed_us =
+                tcp_stamp_us_delta(tp->delivered_mstamp, bbr->cycle_mstamp);
+    u32 inflight, bw;
+    if (bbr->mode != BBR_PROBE_BW)
+        return;
+
+    /* Always need to probe for bw before we forget good bw estimate. */
+    if (elapsed_us > bbr->cycle_len * bbr->min_rtt_us) {
+        /* Start a new PROBE_BW probing cycle of [2 to 8] x min_rtt. */
+        bbr->cycle_mstamp = tp->delivered_mstamp;
+        bbr->cycle_len = CYCLE_LEN - prandom_u32_max(bbr_cycle_rand);
+        bbr_set_cycle_idx(sk, BBR_BW_PROBE_UP);  /* probe bandwidth */
+        return;
+    }
+    /* The pacing_gain of 1.0 paces at the estimated bw to try to fully
+     * use the pipe without increasing the queue.
+     */
+    if (bbr->pacing_gain == BBR_UNIT)
+        return;
+    inflight = rs->prior_in_flight;  /* what was in-flight before ACK? */
+    bw = bbr_max_bw(sk);
+    /* A pacing_gain < 1.0 tries to drain extra queue we added if bw
+     * probing didn't find more bw. If inflight falls to match BDP then we
+     * estimate queue is drained; persisting would underutilize the pipe.
+     */
+    if (bbr->pacing_gain < BBR_UNIT) {
+        if (inflight <= bbr_inflight(sk, bw, BBR_UNIT))
+            bbr_set_cycle_idx(sk, BBR_BW_PROBE_CRUISE); /* cruise */
+        return;
+    }
+    /* A pacing_gain > 1.0 probes for bw by trying to raise inflight to at
+     * least pacing_gain*BDP; this may take more than min_rtt if min_rtt is
+     * small (e.g. on a LAN). We do not persist if packets are lost, since
+     * a path with small buffers may not hold that much. Similarly we exit
+     * if we were prevented by app/recv-win from reaching the target.
+     */
+    if (elapsed_us > bbr->min_rtt_us &&
+            (inflight >= bbr_inflight(sk, bw, bbr->pacing_gain) ||
+            rs->losses ||         /* perhaps pacing_gain*BDP won't fit */
+            rs->is_app_limited || /* previously app-limited */
+            !tcp_send_head(sk) || /* currently app/rwin-limited */
+            !tcp_snd_wnd_test(tp, tcp_send_head(sk), tp->mss_cache))) {
+            bbr_set_cycle_idx(sk, BBR_BW_PROBE_DOWN);  /* drain queue */
+            return;
+    }
+}
+
 /* With pacing at lower layers, there's often less data "in the network" than
  * "in flight". With TSQ and departure time pacing at lower layers (e.g. fq),
  * we often have several skbs queued in the pacing layer with a pre-scheduled
@@ -602,6 +686,11 @@ static void bbr_update_cycle_phase(struct sock *sk,
 {
 	struct bbr *bbr = inet_csk_ca(sk);
 
+    if (bbr_drain_to_target) {
+        bbr_drain_to_target_cycling(sk, rs);
+        return;
+    }
+
 	if (bbr->mode == BBR_PROBE_BW && bbr_is_next_cycle_phase(sk, rs))
 		bbr_advance_cycle_phase(sk);
 }
@@ -1064,6 +1153,7 @@ static void bbr_init(struct sock *sk)
 	bbr->full_bw_cnt = 0;
 	bbr->cycle_mstamp = 0;
 	bbr->cycle_idx = 0;
+	bbr->cycle_len = 0;
 	bbr_reset_lt_bw_sampling(sk);
 	bbr_reset_startup_mode(sk);
 
@@ -1141,7 +1231,7 @@ static void bbr_set_state(struct sock *sk, u8 new_state)
 
 static struct tcp_congestion_ops tcp_bbr_cong_ops __read_mostly = {
 	.flags		= TCP_CONG_NON_RESTRICTED,
-	.name		= "bbr",
+	.name		= "bbrplus",
 	.owner		= THIS_MODULE,
 	.init		= bbr_init,
 	.cong_control	= bbr_main,
